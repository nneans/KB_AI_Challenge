from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
import shutil
import traceback
import fitz  # PyMuPDF
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
from groq import Groq
from langchain.text_splitter import RecursiveCharacterTextSplitter

app = FastAPI(title="KB AI RAG Service")

# CORS í•´ê²° (í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ì„¤ì • ---
GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "your_groq_api_key_here")
EMBEDDING_MODEL_NAME = "jhgan/ko-sroberta-multitask"
GROQ_MODEL_NAME = "llama-3.3-70b-versatile"
COLLECTION_NAME = "local_kb"

# --- ì „ì—­ ë³€ìˆ˜ (ì´ˆê¸°í™”) ---
embedding_model = None
qdrant_client = None
groq_client = None
doc_id_counter = 0

@app.on_event("startup")
async def startup_event():
    global embedding_model, qdrant_client, groq_client
    print("ğŸš€ ì„œë²„ ì‹œì‘: ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # 1. ì„ë² ë”© ëª¨ë¸
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    
    # 2. Qdrant (ë©”ëª¨ë¦¬ ëª¨ë“œ)
    qdrant_client = QdrantClient(":memory:")
    qdrant_client.recreate_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=768, distance=Distance.COSINE),
    )
    
    # 3. Groq
    if GROQ_API_KEY and GROQ_API_KEY != "gsk_...":
        groq_client = Groq(api_key=GROQ_API_KEY)
    else:
        print("âš ï¸ Groq API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")

# --- API ì •ì˜ ---

@app.get("/")
def read_root():
    return {"status": "ok", "message": "KB AI RAG Service is running"}

@app.post("/upload")
async def upload_pdf(files: list[UploadFile] = File(...)):
    global doc_id_counter
    if not qdrant_client or not embedding_model:
        raise HTTPException(status_code=500, detail="Server not initialized")
    
    saved_chunks = 0
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    
    results = []
    
    for file in files:
        try:
            # ì„ì‹œ íŒŒì¼ ì €ì¥ ë° ì½ê¸°
            temp_filename = f"temp_{file.filename}"
            with open(temp_filename, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            doc = fitz.open(temp_filename)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            os.remove(temp_filename) # ì •ë¦¬
            
            if not text.strip():
                results.append({"filename": file.filename, "status": "failed", "reason": "No text extracted"})
                continue

            # ì²­í¬í™” ë° ì„ë² ë”©
            chunks = text_splitter.split_text(text)
            points = []
            for i, chunk in enumerate(chunks):
                vector = embedding_model.encode(chunk).tolist()
                points.append(PointStruct(
                    id=doc_id_counter,
                    vector=vector,
                    payload={"filename": file.filename, "text": chunk}
                ))
                doc_id_counter += 1
            
            if points:
                qdrant_client.upsert(collection_name=COLLECTION_NAME, points=points)
                saved_chunks += len(points)
                results.append({"filename": file.filename, "status": "success", "chunks": len(points)})
                
        except Exception as e:
            traceback.print_exc()
            results.append({"filename": file.filename, "status": "error", "message": str(e)})

    return {"total_chunks": saved_chunks, "details": results}

class ChatRequest(BaseModel):
    query: str

@app.post("/chat")
async def chat(request: ChatRequest):
    if not groq_client:
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë²„ ì„¤ì • ì˜¤ë¥˜(API Key ëˆ„ë½)ì…ë‹ˆë‹¤.", "references": []}
    
    try:
        # 1. ê²€ìƒ‰
        query_vector = embedding_model.encode(request.query).tolist()
        hits = qdrant_client.search(
            collection_name=COLLECTION_NAME,
            query_vector=query_vector,
            limit=5
        )
        
        if not hits:
            return {"answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", "references": []}

        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([h.payload['text'] for h in hits])
        refs = [h.payload['filename'] for h in hits]
        
        # 3. LLM ìƒì„± (Groq)
        system_prompt = "ë‹¹ì‹ ì€ ê¸ˆìœµ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ì°¸ê³ ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì¶œì²˜ë¥¼ ê¼­ ëª…ì‹œí•˜ì„¸ìš”."
        user_prompt = f"ì§ˆë¬¸: {request.query}\n\n[ì°¸ê³ ìë£Œ]\n{context}"
        
        response = groq_client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            model=GROQ_MODEL_NAME,
            temperature=0.1
        )
        
        return {"answer": response.choices[0].message.content, "references": list(set(refs))}
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
