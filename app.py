# =========================================================
# KB ê¸ˆìœµ RAG ì±—ë´‡ (Local Self-Contained Version)
# =========================================================
# ì´ ì½”ë“œëŠ” ì„œë²„ë‚˜ í´ë¼ìš°ë“œ DB ì—†ì´, ì‚¬ìš©ìê°€ ì§ì ‘ PDFë¥¼ ì—…ë¡œë“œí•˜ì—¬
# ë¡œì»¬ì—ì„œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ì§ˆë¬¸í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.
# Groq(LLM), Google(Voice/Translate) APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ë£Œë¡œ ë™ì‘í•©ë‹ˆë‹¤.
# =========================================================

import os
import sys
import numpy as np
import traceback
import fitz  # PyMuPDF (PDF ì²˜ë¦¬)
from typing import List

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import gradio as gr
import speech_recognition as sr
from dotenv import load_dotenv  # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env)

# .env íŒŒì¼ ë¡œë“œ (ë¡œì»¬ ê°œë°œìš©)
load_dotenv()

from deep_translator import GoogleTranslator
from sentence_transformers import SentenceTransformer
from groq import Groq
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError:
    # langchain 0.2.0 ì´ìƒì—ì„œ êµ¬ì¡°ê°€ ë³€ê²½ëœ ê²½ìš°
    from langchain_text_splitters import RecursiveCharacterTextSplitter

# =========================================================
# 1. ì„¤ì • ë° ì´ˆê¸°í™”
# =========================================================

# Groq API í‚¤ (í•„ìˆ˜)
GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "your_groq_api_key_here")
if not GROQ_API_KEY or GROQ_API_KEY == "your_groq_api_key_here":
    print("âš ï¸ GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ ì‚¬ìš© ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ëª¨ë¸ ì„¤ì •
EMBEDDING_MODEL_NAME = "jhgan/ko-sroberta-multitask"
GROQ_MODEL_NAME = "llama-3.3-70b-versatile"
COLLECTION_NAME = "local_kb"

print("ğŸ› ï¸ ëª¨ë¸ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")

# 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë¡œì»¬ ì‹¤í–‰)
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
embedding_model.max_seq_length = 512

# 2. Qdrant í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬ ë©”ëª¨ë¦¬ DB - í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ë°ì´í„° ì‚­ì œë¨)
# ì˜êµ¬ ì €ì¥ì„ ì›í•˜ë©´ path="./local_qdrant_db" ë¡œ ë³€ê²½í•˜ì„¸ìš”.
# ì—¬ê¸°ì„œëŠ” í¬íŠ¸í´ë¦¬ì˜¤ìš© ë°ëª¨ë¥¼ ìœ„í•´ ë§¤ë²ˆ ê¹¨ë—í•œ ìƒíƒœì¸ ':memory:'ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•©ë‹ˆë‹¤.
qdrant_client = QdrantClient(":memory:")

# ì»¬ë ‰ì…˜ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì‚­ì œ í›„ ì¬ìƒì„±)
try:
    qdrant_client.recreate_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=768, distance=Distance.COSINE),
    )
    print(f"âœ… ë¡œì»¬ Qdrant ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ìƒì„± ì™„ë£Œ.")
except Exception as e:
    print(f"âŒ Qdrant ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")

# 3. Groq í´ë¼ì´ì–¸íŠ¸
try:
    groq_client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    groq_client = None
    print(f"âŒ Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

#ì „ì—­ ë³€ìˆ˜: ë¬¸ì„œ ID ì¹´ìš´í„°
doc_id_counter = 0

print("âœ… ëª¨ë“  ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")


# =========================================================
# 2. ë¬¸ì„œ ì²˜ë¦¬ ë° RAG í•µì‹¬ ë¡œì§
# =========================================================

def process_uploaded_files(files):
    """PDF íŒŒì¼ì„ ì½ì–´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë²¡í„° DBì— ì €ì¥"""
    global doc_id_counter
    
    if not files:
        return "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    total_chunks = 0
    status_msg = ""
    
    # í…ìŠ¤íŠ¸ ë¶„ë¦¬ê¸° ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        length_function=len,
    )

    for file in files:
        try:
            # Gradio ë²„ì „/ì„¤ì •ì— ë”°ë¼ fileì´ ë¬¸ìì—´(ê²½ë¡œ)ì¼ ìˆ˜ë„ ìˆê³  ê°ì²´ì¼ ìˆ˜ë„ ìˆìŒ
            file_path = file.name if hasattr(file, 'name') else file
            
            # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            doc = fitz.open(file_path)
            file_text = ""
            for page in doc:
                file_text += page.get_text()
            
            if not file_text.strip():
                status_msg += f"âš ï¸ {os.path.basename(file_path)}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì´ë¯¸ì§€ PDFì¼ ìˆ˜ ìˆìŒ)\n"
                continue
                
            # 2. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)
            chunks = text_splitter.split_text(file_text)
            
            # 3. ì„ë² ë”© ë° ì €ì¥
            points = []
            for i, chunk in enumerate(chunks):
                vector = embedding_model.encode(chunk).tolist()
                
                payload = {
                    "filename": os.path.basename(file_path),
                    "text": chunk,
                    "chunk_id": i
                }
                
                points.append(PointStruct(id=doc_id_counter, vector=vector, payload=payload))
                doc_id_counter += 1
            
            # Qdrantì— ì €ì¥
            if points:
                qdrant_client.upsert(
                    collection_name=COLLECTION_NAME,
                    points=points
                )
                total_chunks += len(points)
                status_msg += f"âœ… {os.path.basename(file_path)}: {len(points)}ê°œ ì§€ì‹ ì €ì¥ ì™„ë£Œ.\n"
            
        except Exception as e:
            traceback.print_exc()
            file_name_debug = getattr(file, 'name', str(file))
            status_msg += f"âŒ {os.path.basename(file_name_debug)} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\n"
            
    print(f"DEBUG: ì´ ì €ì¥ëœ ì²­í¬ ìˆ˜: {total_chunks}")
    if total_chunks == 0:
        return status_msg + "\n(ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. PDFê°€ ë¹„ì–´ìˆê±°ë‚˜ ì´ë¯¸ì§€ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
            
    return f"ì²˜ë¦¬ ì™„ë£Œ! ì´ {total_chunks}ê°œì˜ ì§€ì‹ ì¡°ê°ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n{status_msg}"

def search_knowledge_base(query, top_k=5):
    """ë¡œì»¬ Qdrantì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
    try:
        query_vector = embedding_model.encode(query).tolist()
        # qdrant-client ë²„ì „ì— ë”°ë¼ .search()ê°€ ì—†ê±°ë‚˜ ë‹¤ë¥´ê²Œ ë™ì‘í•  ìˆ˜ ìˆì–´ .query_points() ì‚¬ìš©
        search_result = qdrant_client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=top_k,
            with_payload=True
        )
        return search_result.points
    except Exception as e:
        print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

def generate_answer_groq(query, context_text):
    """Groq APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
    if not groq_client: 
        return "Groq API ì„¤ì • ì˜¤ë¥˜"
        
    system_prompt = """
    ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ê¸ˆìœµ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ [ì°¸ê³ ìë£Œ]ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    ì°¸ê³ ìë£Œì— ë‚´ìš©ì´ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
    ì¶œì²˜(íŒŒì¼ì´ë¦„)ë¥¼ ë‹µë³€ ëì— ëª…ì‹œí•´ì£¼ì„¸ìš”.
    """
    
    user_prompt = f"ì§ˆë¬¸: {query}\n\n[ì°¸ê³ ìë£Œ]\n{context_text}"
    
    try:
        response = groq_client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            model=GROQ_MODEL_NAME,
            temperature=0.1,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Groq ìƒì„± ì˜¤ë¥˜: {e}"

# RAG íŒŒì´í”„ë¼ì¸ (í†µí•©)
def run_rag_pipeline(text_input, detected_lang='ko'):
    if not text_input:
        return "", "", "", ""
        
    # 1. ì§ˆë¬¸ ë²ˆì—­ (í•„ìš”ì‹œ)
    korean_query = text_input
    if detected_lang != 'ko':
        try:
            korean_query = GoogleTranslator(source='auto', target='ko').translate(text_input)
        except: pass

    # 2. ë¬¸ì„œ ê²€ìƒ‰
    hits = search_knowledge_base(korean_query)
    
    if not hits:
        return korean_query, "ì €ì¥ëœ ì§€ì‹ì´ ë¶€ì¡±í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", "", "ì°¸ê³  ë¬¸ì„œ ì—†ìŒ"

    # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = ""
    references = []
    for hit in hits:
        context_text += f"{hit.payload['text']}\n\n"
        references.append(f"- {hit.payload['filename']} (ìœ ì‚¬ë„: {hit.score:.2f})")
    
    ref_str = "\n".join(references)
    
    # 4. ë‹µë³€ ìƒì„±
    korean_answer = generate_answer_groq(korean_query, context_text)
    
    # 5. ë‹µë³€ ë²ˆì—­ (í•„ìš”ì‹œ)
    final_answer = korean_answer
    if detected_lang != 'ko':
        try:
            final_answer = GoogleTranslator(source='ko', target=detected_lang).translate(korean_answer)
        except: pass
        
    return korean_query, korean_answer, final_answer, ref_str


# =========================================================
# 3. ìŒì„± ë° UI í—¬í¼ í•¨ìˆ˜
# =========================================================

def voice_to_text(audio_input):
    """ìŒì„± ì¸ì‹ (Google API)"""
    if audio_input is None: return "ìŒì„± ì…ë ¥ ì—†ìŒ", None
    
    try:
        sample_rate, audio_numpy = audio_input
        if audio_numpy.dtype == np.float32:
            audio_numpy = (audio_numpy * 32767).astype(np.int16)
        if len(audio_numpy.shape) > 1:
            audio_numpy = audio_numpy.mean(axis=1).astype(np.int16)
            
        audio_data = sr.AudioData(audio_numpy.tobytes(), sample_rate, 2)
        r = sr.Recognizer()
        text = r.recognize_google(audio_data, language='ko-KR')
        return text, 'ko'
    except sr.UnknownValueError:
        return "ì¸ì‹ ì‹¤íŒ¨ (ë‹¤ì‹œ ë§í•´ì£¼ì„¸ìš”)", None
    except Exception as e:
        return f"ì˜¤ë¥˜: {e}", None

# =========================================================
# 4. Gradio UI êµ¬ì„±
# =========================================================

# í…Œë§ˆ ì„¤ì • (KB ê¸ˆìœµ ìƒ‰ìƒ - ë…¸ë€ìƒ‰/íšŒìƒ‰ í†¤)
theme = gr.themes.Soft(
    primary_hue="amber",
    neutral_hue="slate",
    font=[gr.themes.GoogleFont("Noto Sans KR"), "ui-sans-serif", "system-ui", "sans-serif"]
)

with gr.Blocks(theme=theme, title="KB Financial AI Assistant") as demo:
    gr.Markdown(
        """
        # ğŸ¦ KB Financial AI Assistant
        **ê¸ˆìœµ ì§€ì‹ RAG ì‹œìŠ¤í…œ**ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.
        
        PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ AIê°€ ë‚´ìš©ì„ í•™ìŠµí•˜ê³ , ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ê³¼ ê·¼ê±° ìë£Œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """
    )
    
    with gr.Accordion("ğŸ“‚ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• (Knowledge Base Setup)", open=True):
        with gr.Row():
            with gr.Column(scale=2):
                file_input = gr.File(label="ë¶„ì„í•  PDF ë¬¸ì„œ ì—…ë¡œë“œ (Drag & Drop)", file_count="multiple", file_types=[".pdf"])
            with gr.Column(scale=1):
                upload_btn = gr.Button("í•™ìŠµ ì‹œì‘ (Build Knowledge Base)", variant="primary")
                upload_status = gr.Textbox(label="ì‹œìŠ¤í…œ ìƒíƒœ", placeholder="ëŒ€ê¸° ì¤‘...", interactive=False)
        
    gr.Markdown("---")
    
    with gr.Row():
        # ì™¼ìª½ ì»¬ëŸ¼: ì…ë ¥ (ìŒì„±/í…ìŠ¤íŠ¸)
        with gr.Column(scale=1, min_width=300):
            gr.Markdown("### ğŸ’¬ ì§ˆë¬¸ ì…ë ¥ (Query)")
            audio_in = gr.Audio(sources=["microphone", "upload"], type="numpy", label="ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°")
            asr_btn = gr.Button("ìŒì„± ì¸ì‹ (STT)", variant="secondary")
            
            text_in = gr.Textbox(label="ì§ˆë¬¸ ë‚´ìš©", placeholder="ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”...", lines=3)
            chat_btn = gr.Button("ë‹µë³€ ìš”ì²­ (Ask AI)", variant="primary", size="lg")
            
        # ì˜¤ë¥¸ìª½ ì»¬ëŸ¼: ê²°ê³¼ (ë‹µë³€/ì°¸ì¡°)
        with gr.Column(scale=2, min_width=400):
            gr.Markdown("### ğŸ¤– ë¶„ì„ ê²°ê³¼ (Analysis Result)")
            answer_box = gr.Textbox(label="AI ë‹µë³€", lines=8, interactive=False, show_copy_button=True)
            ref_box = gr.Textbox(label="ì°¸ê³  ë¬¸í—Œ / ê·¼ê±° ìë£Œ", lines=4, interactive=False)
            
            
    # ì´ë²¤íŠ¸ ì—°ê²°
    upload_btn.click(process_uploaded_files, inputs=[file_input], outputs=[upload_status])
    
    asr_btn.click(voice_to_text, inputs=[audio_in], outputs=[text_in, gr.State()])
    
    chat_btn.click(
        run_rag_pipeline, 
        inputs=[text_in, gr.State('ko')], # ì–¸ì–´ëŠ” ê¸°ë³¸ í•œêµ­ì–´ë¡œ ê³ ì • (ë‹¨ìˆœí™”)
        outputs=[gr.State(), answer_box, gr.State(), ref_box]
    )

if __name__ == "__main__":
    demo.launch(share=True)
